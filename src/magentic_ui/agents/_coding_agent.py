import asyncio
from pathlib import Path
import shutil
from typing import AsyncGenerator, List, Sequence, Optional
import re
from typing import Any, Mapping
import uuid
from autogen_core.tools import Workbench
from loguru import logger
from datetime import datetime
from pydantic import Field
from autogen_core import CancellationToken, ComponentModel, Component
from autogen_core.models import (
    ChatCompletionClient,
    UserMessage,
    SystemMessage,
)
from pydantic import BaseModel
from typing_extensions import Self

from autogen_agentchat.agents import BaseChatAgent, AssistantAgent
from autogen_core.code_executor import CodeBlock, CodeExecutor
from autogen_core.model_context import (
    ChatCompletionContext,
    TokenLimitedChatCompletionContext,
)
from autogen_agentchat.base import Response
from autogen_agentchat.state import BaseState
from autogen_agentchat.messages import (
    BaseAgentEvent,
    BaseChatMessage,
    TextMessage,
    ToolCallExecutionEvent,
    MessageFactory,
    ToolCallRequestEvent,
    BaseTextChatMessage,
)
from autogen_ext.code_executors.local import LocalCommandLineCodeExecutor

from ..utils import thread_to_context

from ..approval_guard import BaseApprovalGuard
from ..guarded_action import ApprovalDeniedError, TrivialGuardedAction
from ..tools.mcp import AggregateMcpWorkbench, NamedMcpServerParams
from ..docker_manager import DockerManager

'''
def _extract_markdown_code_blocks(markdown_text: str) -> List[CodeBlock]:
    pattern = re.compile(r"```(?:\s*([\w\+\-]+))?\n([\s\S]*?)```")
    matches = pattern.findall(markdown_text)
    code_blocks: List[CodeBlock] = []
    for match in matches:
        language = match[0].strip() if match[0] else ""
        code_content = match[1]
        code_blocks.append(CodeBlock(code=code_content, language=language))
    return code_blocks


async def _invoke_action_guard(
    thread: Sequence[BaseChatMessage | BaseAgentEvent],
    delta: Sequence[BaseChatMessage | BaseAgentEvent],
    code_message: TextMessage,
    agent_name: str,
    model_client: ChatCompletionClient,
    approval_guard: BaseApprovalGuard | None,
) -> None:
    # Get approval for the coding request. We could conceivably do extra work to enable interactive approval here,
    # but the value for many users is likely to be low, as it may not be appropriate to assume knowledge of coding,
    # and thus the user will not have the context necessary to approve/deny the incremental execution of code blocks.
    guarded_action = TrivialGuardedAction("coding", baseline_override="maybe")

    # Note that delta already contains the code message.
    assert delta[-1] == code_message

    thread = list(thread) + list(delta)

    context = thread_to_context(
        thread,
        agent_name,
        is_multimodal=model_client.model_info["vision"],
    )
    action_description_for_user = TextMessage(
        content="Do you want to execute the code above?",
        source=agent_name,
    )

    await guarded_action.invoke_with_approval(
        {}, code_message, context, approval_guard, action_description_for_user
    )
'''

async def _coding(
    system_prompt: str,
    inner_messages: List[BaseAgentEvent | BaseChatMessage],
    thread: Sequence[BaseChatMessage | BaseAgentEvent],
    agent_name: str,
    model_client: ChatCompletionClient,
    workbench: Workbench,
    cancellation_token: CancellationToken,
    model_context: ChatCompletionContext,
    ) -> AsyncGenerator[BaseAgentEvent | BaseChatMessage | Response, None]:
    """Write code using the model and executor.

    It calls the workbench to generate code based on the system prompt and the thread of messages.

    When the cancellation token is set, the execution will stop.
    Args:
        system_prompt (str): The system prompt to guide the model.
        thread (Sequence[BaseChatMessage]): The thread of messages to use as context.
        agent_name (str): The name of the agent.
        model_client (ChatCompletionClient): The model client to use for cod    # extract code blocks from the LLM's response
    code_msg = TextMessage(
        source=agent_name + "-llm",
        metadata={"internal": "no", "type": "potential_code"},
        content=create_result.content,
    )
    # add LLM's response to the current thread.
    delta.append(code_msg)
    yield code_msge generation.
        workbench (Workbench): The workbench to use for generating code.
        max_debug_rounds (int): The maximum number of debug rounds to perform.
        cancellation_token (CancellationToken): The cancellation token to stop execution.
        model_context (ChatCompletionContext): The context for the model.

    Yields:
        TextMessage: The intermediate messages generated by the model and executor.
        bool: A flag indicating whether any code execution was performed.

    Raises:
        ApprovalDeniedError: If the user denies the approval for the coding request.
    """
    # The list of new messages to be added to the thread.
    delta: Sequence[BaseChatMessage | BaseAgentEvent] = []

    # Add system prompt as the last message before generation
    current_thread = (
        list(thread)
        + list(delta)
    )

    # create an LLM context from system message, global chat history, and inner messages
    context = [SystemMessage(content=system_prompt)] + thread_to_context(
        current_thread,
        agent_name,
        is_multimodal=model_client.model_info["vision"],
    )

    # Re-initialize model context to meet token limit quota
    try:
        await model_context.clear()
        for msg in context:
            await model_context.add_message(msg)
        token_limited_context = await model_context.get_messages()
    except Exception:
        token_limited_context = context
        
    tools = await workbench.list_tools()
    # Generate code using the model.
    create_result = await model_client.create(
        messages=token_limited_context, cancellation_token=cancellation_token, tools=tools
    )
        
    # events and msgs produced during the '_process_model_result'
    async for output_result in AssistantAgent._process_model_result( # type: ignore
        model_result=create_result,
        inner_messages=inner_messages,
        cancellation_token=cancellation_token,
        agent_name=agent_name,
        system_messages=[SystemMessage(content=system_prompt)],
        model_context=model_context,
        workbench=[workbench],
        handoff_tools=[],
        handoffs={},
        model_client=model_client,
        model_client_stream=False,
        reflect_on_tool_use=False,
        max_tool_iterations=1,
        tool_call_summary_format="{result}",
        tool_call_summary_formatter=None,
        output_content_type=None,
        message_id=str(uuid.uuid4()),
    ):
        yield output_result       

'''
async def _summarize_coding(
    agent_name: str,
    model_client: ChatCompletionClient,
    thread: Sequence[BaseChatMessage | BaseAgentEvent],
    cancellation_token: CancellationToken,
    model_context: ChatCompletionContext,
) -> TextMessage:
    # Create a summary from the inner messages using an extra LLM call.
    input_messages = (
        [SystemMessage(content="你是一个会写代码和debug代码的智能体")]
        + thread_to_context(
            list(thread), agent_name, is_multimodal=model_client.model_info["vision"]
        )
        + [
            UserMessage(
                content="""
                上述文本是你最初收到的请求和你的历史消息。
                你需要为当前发生的所有的事情都生成一个总结概要，然后基于这些总结回答你收到的请求。
                如果过程中有代码被执行，请复制最终没有错误的代码。
                不要再赘述"你正在总结"，直接给出总结内容。""",
                source="user",
            )
        ]
    )

    # Re-initialize model context to meet token limit quota
    try:
        await model_context.clear()
        for msg in input_messages:
            await model_context.add_message(msg)
        token_limited_input_messages = await model_context.get_messages()
    except Exception:
        token_limited_input_messages = input_messages

    summary_result = await model_client.create(
        messages=token_limited_input_messages, cancellation_token=cancellation_token
    )
    assert isinstance(summary_result.content, str)
    code_block_list = _extract_markdown_code_blocks(summary_result.content)
    assert isinstance(summary_result.content, str)
    return TextMessage(
        source=agent_name,
        metadata={"internal": "yes", "has_codeblocks": "yes" if len(code_block_list) == 0 else "no"},
        content=summary_result.content,
    )
'''

class CodingAgentConfig(BaseModel):
    name: str
    model_client: ComponentModel
    description: str = """
    一个可以写代码和执行代码的智能体。它可以使用它的语言能力来总结、写代码、解决数学和逻辑问题。它可以理解图像，并使用它们来帮助它完成任务。
    它可以访问文件，如果给出路径，它可以使用python代码来操作它们。如果你想要操作文件或读取csv或excel文件，你可以使用这个智能体。
    当你要求智能体完成某件事：它会写代码，然后立即执行代码。如果出现错误，它可以调试代码并再次尝试。
    """
    app_dir: Path
    max_debug_rounds: int = 3
    summarize_output: bool = False
    coding_tools: List[NamedMcpServerParams]
    # Optionally add code_executor config if needed


class CodingAgentState(BaseState):
    chat_history: List[BaseChatMessage] = Field(default_factory=list[BaseChatMessage])
    type: str = Field(default="CodingAgentState")


class CodingAgent(BaseChatAgent, Component[CodingAgentConfig]):
    """An coding agent capable of writing, generating code and save the code to files.

    The agent uses either Docker-based code generator to generate code
    in a controlled environment. It maintains a chat history and can be paused/resumed
    during generation.
    """

    component_type = "agent"
    component_config_schema = CodingAgentConfig
    component_provider_override = "magentic_ui.agents.CodingAgent"
    coding_workbench: AggregateMcpWorkbench

    DEFAULT_DESCRIPTION = """
    一个可以写代码的智能体。它可以使用它的语言能力和代码工具来分析你的代码需求、生成代码、保存代码到文件。
    它也可以基于现有的代码，按照需求进行修改、优化、重构。
    请将任何代码相关的任务交给此智能体。
    """

    system_prompt_coding_agent_template = """
    
    你是一个强大的智能体助手。你会回答用户的问题和请求。
    你的主要工作是为用户写代码、优化代码、解释代码、重构代码、修复代码问题(bug)、回答代码相关的问题等任何和代码有关的任务。
    你装备有一个强大的代码工具gemini-cli，它懂得ST、C/C++、Java、Python、Shell、SQL、HTML、CSS、JavaScript、TypeScript、React、Vue、Angular、Node.js、Express、Django、Flask、FastAPI、MySQL、PostgreSQL、SQLite等几乎所有常见的编程语言，你可以使用该工具帮助你完成任何代码相关的任务。
    你需要分析用户的需求，明确用户的核心需求，如果存在不明确的需求，你需要向用户询问清楚，直到用户的需求明确为止或者用户拒绝回答或者用户自己也不明确自己的需求。
    视用户需求的简易程度，你可以直接用自己的知识为用户生成满足需求的代码或者使用代码工具实现复杂的代码需求。
    对于简单的代码任务，你可以不使用代码工具，直接使用你自己的语言能力来生成代码。

    今天是：{date_today}

    <需求分析>
    关于用户的需求的，你需要谨记并遵循如下规定：
    - 你需要优先分析的需求，而不是直接生成代码。
    - 如果存在不明确的需求，你需要向用户询问清楚，直到用户的需求明确为止或者用户拒绝回答或者用户自己也不明确自己的需求。
    - 如果用户拒绝回答或者用户自己也不明确自己的需求，你可以根据已知的需求自行决定最可能满足用户需求的方案，并将你的决定告知用户。
    - 你可以在必要时引导用户明确他们的需求，比如用户向你询问帮他绝对需求时，你可以将相关的典型需求一一列举并带上序号以便用户通过序号选择，并询问用户是否需要这些需求。
    - 不要频繁的向用户询问需求，总是优先尝试用自己的知识解决问题，除非用户明确表示需要你的帮助。
    - 一定以用户需求为导向和核心，不要试图猜测用户的需求，不要偏离用户要求。
    
    * 例子只是为了给抽象的语言描述做一个大致的具体说明，实际上你需要根据上下文和自身的知识做出最合适的回答。
    
    例子1:用户没有提出明确的功能需求，且自己也不知道，你向用户提问后，根据用户的回答实现用户想要的程序。
      用户：帮我开发一个网站的程序。
      你：请问您想用什么框架开发网站的前后端？常用的网站前端框架有:
          1. React
          2. Vue
          3. Angular
          常用的网站后端框架有:
          1. Django
          2. Flask
          3. FastAPI
          4. Spring Boot
          你可以直接告诉我你想要用编程语言或者用编号告诉我。如果你想用的框架不在上述列表中，也可以直接告诉我。
      用户:前端用1。
      你:好的，我将为为您使用React开发的网站的前端。因为你没有选择后端框架，我将为您使用Fastapi开发的网站的后端。
    
    </需求分析>
    
    <代码生成>
    关于代码生成，你需要谨记并遵循如下规定：
    - 对于简单的代码需求，你可以直接用自己的知识为用户生成满足需求的代码，而不需要使用额外的代码工具gemini-cli。
    - 除简单的代码需求外，你总是使用代码工具gemini-cli来完成用户的其他代码需求。
    - 使用代码工具gemini-cli时，你不需要为用户生成任何代码，所有的代码都将由代码工具生成。
    - 代码工具gemini-cli生成的代码，可能直接保存在文件中，此时你只需要告诉用户代码已经生成，并告诉用户文件路径。
    
    简单的代码需求例子：
    1. 代码语法等基础的使用问题，存在客观标准答案，比如，"如何在Python打印", "如何定义一个C++类"。
    2. 实现一个简单的只需要一段代码就能实现的功能，比如，"用户python", "写一个函数，实现一个简单的计算器，可以计算加减乘除"。
    </代码生成>
    
    <与用户交流>
    - 当与用户交流时，优化你的写作以提高清晰度和可读性。  
    - 不要在代码中添加冗余的叙述性注释来解释代码的功能。 
    </与用户交流>
    
    <markdown格式>
    生成markdown的规则:
    - 用户喜欢你使用'###'和'##'标题来组织消息。不要使用'#'标题，因为用户觉得它们渲染后视觉上大的失调。
    - 使用粗体markdown（**文本**）来突出消息中的关键信息，例如特定问题的答案或关键见解。
    - 列表项应该使用'- '而非'• '，同时也应该使用粗体markdown作为伪标题，特别是如果有子列表项。列表项'- item: description'需要转换为为使用粗体markdown的格式，比如:'- **item**: description'。
    - 当提及文件、目录、类或函数时，使用反引号来格式化它们。例如`app/components/Card.tsx`。
    - 当提及URL时，不要直接裸用URL。总是使用反引号或markdown链接。当有描述性的锚文本时，优先使用markdown链接；否则将URL包裹在反引号中（例如`https://example.com`）。
    - 如果存在数学表达式，且不属于代码的一部分，使用内联数学（\\(和\\)）或块数学（\\[和\\]）来格式化它。
    </markdown格式>
    
    额外提醒:
    - 你的任务不总是要求写代码，比如，写一首诗，解释一段文字等。这种情况下，不要生成代码或者调用代码工具，直接回答。
    - 当被要求解决数学或逻辑问题时，首先尝试不写代码直接回答，只有在有必要的时候才使用代码（优先使用Pyton）来解决这类问题。

    """

    def __init__(
        self,
        name: str,
        model_client: ChatCompletionClient,
        coding_tools: List[NamedMcpServerParams],
        app_dir: Path,
        model_context_token_limit: int = 128000,
        description: str = DEFAULT_DESCRIPTION,
        max_debug_rounds: int = 3,
        summarize_output: bool = False,
        code_executor: Optional[CodeExecutor] = None,
        use_local_executor: bool = False,
        approval_guard: BaseApprovalGuard | None = None,
    ) -> None:
        """Initialize the CodingAgent.

        Args:
            name (str): The name of the agent
            model_client (ChatCompletionClient): The language model client to use.
            coding_tools: the NamedMcpServerParams specifying the coding provider MCP server.
            description (str, optional): Description of the agent's capabilities. Default: DEFAULT_DESCRIPTION.
            max_debug_rounds (int, optional): Maximum number of code debugging iterations. Default: 3.
            summarize_output (bool, optional): Whether to summarize code execution results. Default: False.
            code_executor (Optional[CodeExecutor], optional): It does not execute code curerenly, 
                but utilize the code_executor to run the coding provider MCP server. Default: None.
            work_dir (Path | str | None, optional): Working directory for code execution. Default: None.
            bind_dir (Path | str | None, optional): Directory to bind for Docker executor. Default: None.
            use_local_executor (bool, optional): Whether to use local instead of Docker executor. Default: False.
        """
        super().__init__(name, description)
        self._model_client = model_client
        self._model_context = TokenLimitedChatCompletionContext(
            model_client, token_limit=model_context_token_limit
        )
        self._chat_history: List[BaseChatMessage | BaseAgentEvent] = []
        self._max_debug_rounds = max_debug_rounds
        self._summarize_output = summarize_output
        self.is_paused = False
        self._paused = asyncio.Event()
        self._approval_guard = approval_guard
        self._did_lazy_init = False
        self.app_dir = app_dir
        self._work_dir = app_dir / "files" / "coding"
        self._cleanup_work_dir = False
        
        self._coding_tool_available = False
        if code_executor:
            self._code_executor = code_executor
        elif use_local_executor:
            self._code_executor = LocalCommandLineCodeExecutor(work_dir=self._work_dir)
        else:
            from .._docker import CODING_IMAGE

            name = f"{name}-{uuid.uuid4()}"
            container_name = "gemini_mcp"
            gemini_mcp_volume = {"/extrahome/lx/2025/magentic-ui/mcp_servers/gemini-cli": {"bind": "/data/gemini-cli", "mode": "rw"}}
            self._code_executor = DockerManager(
                image=CODING_IMAGE,
                container_name=container_name,
                working_dir="/data/gemini-cli",
                volumes={str(self._work_dir): {"bind": "/data/gemini-cli/generate", "mode": "rw"}, **gemini_mcp_volume},
                ports={"18100": "18100"},
                delete_tmp_files=True,
                init_command="bash -c 'source /data/gemini-cli/run.sh'",
                detach=True,
                tty=True,
                auto_remove=False,
            )
        
        self.coding_workbench = AggregateMcpWorkbench(named_server_params=coding_tools)
            
    async def lazy_init(self) -> None:
        """Initialize the code executor if it has a start method.

        This method is called after initialization to set up any async resources
        needed by the code executor.
        """
        if self._did_lazy_init:
            return
            
        # Start the container if it needs to be started
        if self._code_executor:
            try:
                logger.info(f"Starting code executor container for coding agent '{self.name}'...")
                if hasattr(self._code_executor, "start"):
                    if not await self._code_executor.start():  # type: ignore
                        raise RuntimeError(f"Failed to start code executor container for coding agent '{self.name}'")
                self._coding_tool_available = True
                logger.info(f"Successfully started code executor container for coding agent '{self.name}'")
            except Exception as e:
                error_msg = f"Failed to start code executor container for coding agent '{self.name}': {str(e)}"
                logger.error(error_msg)
                self._coding_tool_available = False
                # Don't raise the exception here, let the agent continue but mark it as failed
                
        self._did_lazy_init = True

    async def close(self) -> None:
        """Clean up resources used by the agent.

        This method:
        - Stops the code executor
        - Removes the work directory if it was created
        - Closes the model client
        """
        logger.info("Closing CodingAgent...")
        # Remove the work directory if it was created.
        if self._cleanup_work_dir and self._work_dir.exists():
            await asyncio.to_thread(shutil.rmtree, self._work_dir)
        # Close the model client.
        await self._model_client.close()

    async def pause(self) -> None:
        """Pause the agent by setting the paused state."""
        self.is_paused = True
        self._paused.set()

    async def resume(self) -> None:
        """Resume the agent by clearing the paused state."""
        self.is_paused = False
        self._paused.clear()

    @property
    def produced_message_types(self) -> Sequence[type[BaseChatMessage]]:
        """Get the types of messages produced by the agent."""
        return (TextMessage,)

    async def on_messages(
        self, messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken
    ) -> Response:
        """Handle incoming messages and return a single response. Calls the on_messages_stream."""
        response: Response | None = None
        async for message in self.on_messages_stream(messages, cancellation_token):
            if isinstance(message, Response):
                response = message
        assert response is not None
        return response

    async def on_messages_stream(
        self, messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken
    ) -> AsyncGenerator[BaseAgentEvent | BaseChatMessage | Response, None]:
        """Handle incoming messages and yield responses as a stream. Append the request to agents chat history."""
        await self.lazy_init()

        # Check if container failed to start
        if not self._coding_tool_available:
            yield Response(
                chat_message=TextMessage(
                    content=f"代码助手无法访问代码工具，无法执行任何代码生成相关的任务。",
                    source=self.name,
                    metadata={"internal": "no"},
                )
            )
            return

        if self.is_paused:
            yield Response(
                chat_message=TextMessage(
                    content="代码助手已暂停。",
                    source=self.name,
                    metadata={"internal": "yes"},
                )
            )
            return
        self._chat_history.extend(messages)
        last_message_received: BaseChatMessage = messages[-1]
        inner_messages: List[BaseAgentEvent | BaseChatMessage] = []

        # Set up the cancellation token for the code execution.
        code_execution_token = CancellationToken()

        # Cancel the code execution if the handler's cancellation token is set.
        cancellation_token.add_callback(lambda: code_execution_token.cancel())

        # Set up background task to monitor the pause event and cancel the code execution if paused.
        async def monitor_pause() -> None:
            await self._paused.wait()
            code_execution_token.cancel()

        monitor_pause_task = asyncio.create_task(monitor_pause())

        system_prompt_coding_agent = self.system_prompt_coding_agent_template.format(
            date_today=datetime.now().strftime("%Y-%m-%d")
        )

        try:
            # Run the code execution and debugging process.
            async for msg in _coding(
                system_prompt=system_prompt_coding_agent,
                inner_messages=inner_messages,
                thread=self._chat_history,
                agent_name=self.name,
                model_client=self._model_client,
                workbench=self.coding_workbench,
                cancellation_token=code_execution_token,
                model_context=self._model_context,
            ):
            # Display some messages to the UI by setting event.metadata = {"internal": False}
                if isinstance(
                    msg,
                    (
                        BaseTextChatMessage,
                        ToolCallRequestEvent,
                        ToolCallExecutionEvent,
                        # ToolCallSummaryMessage,
                    ),
                ):
                    metadata = getattr(msg, "metadata", {})
                    metadata = {
                        **metadata,
                        # Display in UI
                        "internal": "no",
                        # Part of a plan step
                        "type": "progress_message",
                    }
                    setattr(msg, "metadata", metadata)

                yield msg
            
        except ApprovalDeniedError:
            # If the user denies the approval, we respond with a message.
            yield Response(
                chat_message=TextMessage(
                    content="用户拒绝了执行代码。",
                    source=self.name,
                    metadata={"internal": "no"},
                ),
                inner_messages=inner_messages,
            )
        except asyncio.CancelledError:
            # If the task is cancelled, we respond with a message.
            yield Response(
                chat_message=TextMessage(
                    content="当前任务被用户取消。",
                    source=self.name,
                    metadata={"internal": "yes"},
                ),
                inner_messages=inner_messages,
            )
        except Exception as e:
            logger.error(f"Error in CodingAgent: {e}")
            # add to chat history
            self._chat_history.append(
                TextMessage(
                    content=f"生成代码时发生错误： {e}",
                    source=self.name,
                )
            )
            yield Response(
                chat_message=TextMessage(
                    content=f"coding智能体发生如下错误： {e}",
                    source=self.name,
                    metadata={"internal": "no"},
                ),
                inner_messages=inner_messages,
            )
        finally:
            # Cancel the monitor task.
            try:
                monitor_pause_task.cancel()
                await monitor_pause_task
            except asyncio.CancelledError:
                pass

    async def on_reset(self, cancellation_token: CancellationToken) -> None:
        """Clear the chat history."""
        self._chat_history.clear()

    def _to_config(self) -> CodingAgentConfig:
        """Convert the agent's state to a configuration object."""
        return CodingAgentConfig(
            name=self.name,
            model_client=self._model_client.dump_component(),
            app_dir=self.app_dir,
            coding_tools=self.coding_workbench.server_params,
            description=self.description,
            max_debug_rounds=self._max_debug_rounds,
            summarize_output=self._summarize_output,
            # TODO: Optionally add code_executor configuration if supported
        )

    @classmethod
    def _from_config(cls, config: CodingAgentConfig) -> Self:
        """Create an agent instance from a configuration object."""
        return cls(
            name=config.name,
            model_client=ChatCompletionClient.load_component(config.model_client),
            app_dir=config.app_dir,
            coding_tools=config.coding_tools,  # Convert single tool to list
            description=config.description,
            max_debug_rounds=config.max_debug_rounds,
            summarize_output=config.summarize_output,
            # TODO: Optionally load code_executor from config if provided
        )

    async def save_state(self) -> Mapping[str, Any]:
        """
        Save the state of the agent.
        """
        return {
            "chat_history": [msg.dump() for msg in self._chat_history],
        }

    async def load_state(self, state: Mapping[str, Any]) -> None:
        """
        Load the state of the agent.
        """
        # Create message factory for deserialization.
        message_factory = MessageFactory()
        self._chat_history = []
        for msg_data in state["chat_history"]:
            msg = message_factory.create(msg_data)
            assert isinstance(msg, BaseChatMessage)
            self._chat_history.append(msg)
